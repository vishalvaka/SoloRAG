{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c43358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishalvaka/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2025-06-16 16:32:39.951620: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-16 16:32:39.970867: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750116759.995516    1662 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750116760.001985    1662 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750116760.015069    1662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750116760.015085    1662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750116760.015087    1662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750116760.015088    1662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-16 16:32:40.018157: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os, json, re, textwrap, pathlib\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import requests, httpx, asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss                 # CPU build is fine for <1 M vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb76bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL   = \"http://localhost:11434\"\n",
    "LLM_MODEL    = \"llama3:8b-instruct-q5_K_M\"          # fits 12 GB VRAM\n",
    "EMBED_MODEL  = \"nomic-embed-text\"                   # pull via SentenceTxf\n",
    "DATA_DIR     = pathlib.Path(\"../data\")\n",
    "ART_DIR      = pathlib.Path(\"../artifacts\")\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "ART_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d015593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, pathlib, time\n",
    "from xml.etree import ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2672e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT          = \"https://support.stripe.com\"\n",
    "MASTER_SITEMAP= f\"{ROOT}/sitemap.xml\"\n",
    "HEADERS       = {\n",
    "    \"User-Agent\":      \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "PARA_MIN_WORDS = 20          # drop boiler-plate <p> shorter than this\n",
    "DELAY_SEC      = 0.4         # polite delay between HTTP requests\n",
    "OUT_DIR        = pathlib.Path(\"data/raw\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_JSONL      = OUT_DIR / \"stripe_faqs_full.jsonl\"\n",
    "OUT_ART_INDEX  = OUT_DIR / \"stripe_article_index.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c64f19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, pathlib, time, re, xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "###############################################################################\n",
    "# Configuration\n",
    "###############################################################################\n",
    "ROOT          = \"https://support.stripe.com\"\n",
    "ROBOTS_TXT    = f\"{ROOT}/robots.txt\"\n",
    "HEADERS       = {\n",
    "    \"User-Agent\":      \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "PARA_MIN_WORDS = 20\n",
    "DELAY_SEC      = 0.35                         # polite, keeps total run < 5 min\n",
    "\n",
    "OUT_DIR        = pathlib.Path(\"data/raw\");  OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_JSONL      = OUT_DIR / \"stripe_faqs_full.jsonl\"\n",
    "OUT_ART_INDEX  = OUT_DIR / \"stripe_article_index.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6285e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url: str) -> str:\n",
    "    r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e07f54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sitemap_urls() -> list[str]:\n",
    "    \"\"\"Read robots.txt and return every absolute sitemap URL.\"\"\"\n",
    "    print(\"ðŸ”  Reading robots.txt â€¦\")\n",
    "    robots = fetch(ROBOTS_TXT)\n",
    "    sm_urls = []\n",
    "    for line in robots.splitlines():\n",
    "        if line.lower().startswith(\"sitemap:\"):\n",
    "            url = line.split(\":\", 1)[1].strip()\n",
    "            if url.startswith(\"/\"):          # rarely happens\n",
    "                url = ROOT + url\n",
    "            sm_urls.append(url)\n",
    "    if not sm_urls:\n",
    "        raise RuntimeError(\"No Sitemap lines found in robots.txt â€“ \"\n",
    "                           \"Stripe may have changed their layout.\")\n",
    "    print(f\"   Found {len(sm_urls)} sitemap file(s)\")\n",
    "    return sm_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0f1faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_article_urls(sm_urls: list[str]):\n",
    "    \"\"\"Yield every /questions/* URL from all sitemap files.\"\"\"\n",
    "    q_pat = re.compile(r\"/questions/\")\n",
    "    for sm in sm_urls:\n",
    "        xml = fetch(sm)\n",
    "        root = ET.fromstring(xml)\n",
    "        for loc in root.findall(\".//{*}loc\"):\n",
    "            url = loc.text.strip()\n",
    "            if q_pat.search(url):\n",
    "                yield url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e1c1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl():\n",
    "    sitemap_urls = get_sitemap_urls()\n",
    "    article_urls = sorted(set(iter_article_urls(sitemap_urls)))\n",
    "    print(f\"   Discovered {len(article_urls)} article URLs\")\n",
    "\n",
    "    docs = []\n",
    "    for i, url in enumerate(article_urls, 1):\n",
    "        html = fetch(url)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        title_tag = soup.select_one(\"h1\")\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"Untitled\"\n",
    "\n",
    "        for p in soup.find_all(\"p\"):\n",
    "            txt = p.get_text(\" \", strip=True)\n",
    "            if len(txt.split()) >= PARA_MIN_WORDS:\n",
    "                docs.append({\"url\": url, \"title\": title, \"text\": txt})\n",
    "\n",
    "        if i % 25 == 0 or i == len(article_urls):\n",
    "            print(f\"   {i:>3}/{len(article_urls)} pages done \"\n",
    "                  f\"({len(docs)} paragraphs)\")\n",
    "\n",
    "        time.sleep(DELAY_SEC)\n",
    "\n",
    "    # save outputs\n",
    "    OUT_JSONL.write_text(\"\\n\".join(json.dumps(d, ensure_ascii=False) for d in docs),\n",
    "                         encoding=\"utf-8\")\n",
    "    OUT_ART_INDEX.write_text(json.dumps(article_urls, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\nâœ…  Finished.\")\n",
    "    print(f\"   Pages crawled  : {len(article_urls)}\")\n",
    "    print(f\"   Paragraphs kept: {len(docs)}\")\n",
    "    print(f\"   JSONL saved to : {OUT_JSONL.relative_to(pathlib.Path.cwd())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37e1444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”  Reading robots.txt â€¦\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No Sitemap lines found in robots.txt â€“ Stripe may have changed their layout.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m, in \u001b[0;36mcrawl\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrawl\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     sitemap_urls \u001b[38;5;241m=\u001b[39m \u001b[43mget_sitemap_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     article_urls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(iter_article_urls(sitemap_urls)))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Discovered \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(article_urls)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m article URLs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mget_sitemap_urls\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         sm_urls\u001b[38;5;241m.\u001b[39mappend(url)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sm_urls:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Sitemap lines found in robots.txt â€“ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStripe may have changed their layout.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sm_urls)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sitemap file(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sm_urls\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No Sitemap lines found in robots.txt â€“ Stripe may have changed their layout."
     ]
    }
   ],
   "source": [
    "crawl()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
