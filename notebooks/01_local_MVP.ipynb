{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c43358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishalvaka/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2025-06-16 16:32:39.951620: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-16 16:32:39.970867: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750116759.995516    1662 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750116760.001985    1662 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750116760.015069    1662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750116760.015085    1662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750116760.015087    1662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750116760.015088    1662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-16 16:32:40.018157: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os, json, re, textwrap, pathlib\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import requests, httpx, asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss                 # CPU build is fine for <1 M vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb76bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL   = \"http://localhost:11434\"\n",
    "LLM_MODEL    = \"llama3:8b-instruct-q5_K_M\"          # fits 12 GB VRAM\n",
    "EMBED_MODEL  = \"nomic-embed-text\"                   # pull via SentenceTxf\n",
    "DATA_DIR     = pathlib.Path(\"../data\")\n",
    "ART_DIR      = pathlib.Path(\"../artifacts\")\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "ART_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d015593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, pathlib, time\n",
    "from xml.etree import ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2672e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT          = \"https://support.stripe.com\"\n",
    "MASTER_SITEMAP= f\"{ROOT}/sitemap.xml\"\n",
    "HEADERS       = {\n",
    "    \"User-Agent\":      \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "PARA_MIN_WORDS = 20          # drop boiler-plate <p> shorter than this\n",
    "DELAY_SEC      = 0.4         # polite delay between HTTP requests\n",
    "OUT_DIR        = pathlib.Path(\"data/raw\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_JSONL      = OUT_DIR / \"stripe_faqs_full.jsonl\"\n",
    "OUT_ART_INDEX  = OUT_DIR / \"stripe_article_index.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c64f19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, pathlib, time, re, xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "###############################################################################\n",
    "# Configuration\n",
    "###############################################################################\n",
    "ROOT          = \"https://support.stripe.com\"\n",
    "ROBOTS_TXT    = f\"{ROOT}/robots.txt\"\n",
    "HEADERS       = {\n",
    "    \"User-Agent\":      \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "PARA_MIN_WORDS = 20\n",
    "DELAY_SEC      = 0.35                         # polite, keeps total run < 5 min\n",
    "\n",
    "OUT_DIR        = pathlib.Path(\"data/raw\");  OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_JSONL      = OUT_DIR / \"stripe_faqs_full.jsonl\"\n",
    "OUT_ART_INDEX  = OUT_DIR / \"stripe_article_index.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6285e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url: str) -> str:\n",
    "    r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e07f54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sitemap_urls() -> list[str]:\n",
    "    \"\"\"Read robots.txt and return every absolute sitemap URL.\"\"\"\n",
    "    print(\"🔍  Reading robots.txt …\")\n",
    "    robots = fetch(ROBOTS_TXT)\n",
    "    sm_urls = []\n",
    "    for line in robots.splitlines():\n",
    "        if line.lower().startswith(\"sitemap:\"):\n",
    "            url = line.split(\":\", 1)[1].strip()\n",
    "            if url.startswith(\"/\"):          # rarely happens\n",
    "                url = ROOT + url\n",
    "            sm_urls.append(url)\n",
    "    if not sm_urls:\n",
    "        raise RuntimeError(\"No Sitemap lines found in robots.txt – \"\n",
    "                           \"Stripe may have changed their layout.\")\n",
    "    print(f\"   Found {len(sm_urls)} sitemap file(s)\")\n",
    "    return sm_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0f1faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_article_urls(sm_urls: list[str]):\n",
    "    \"\"\"Yield every /questions/* URL from all sitemap files.\"\"\"\n",
    "    q_pat = re.compile(r\"/questions/\")\n",
    "    for sm in sm_urls:\n",
    "        xml = fetch(sm)\n",
    "        root = ET.fromstring(xml)\n",
    "        for loc in root.findall(\".//{*}loc\"):\n",
    "            url = loc.text.strip()\n",
    "            if q_pat.search(url):\n",
    "                yield url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e1c1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl():\n",
    "    sitemap_urls = get_sitemap_urls()\n",
    "    article_urls = sorted(set(iter_article_urls(sitemap_urls)))\n",
    "    print(f\"   Discovered {len(article_urls)} article URLs\")\n",
    "\n",
    "    docs = []\n",
    "    for i, url in enumerate(article_urls, 1):\n",
    "        html = fetch(url)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        title_tag = soup.select_one(\"h1\")\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"Untitled\"\n",
    "\n",
    "        for p in soup.find_all(\"p\"):\n",
    "            txt = p.get_text(\" \", strip=True)\n",
    "            if len(txt.split()) >= PARA_MIN_WORDS:\n",
    "                docs.append({\"url\": url, \"title\": title, \"text\": txt})\n",
    "\n",
    "        if i % 25 == 0 or i == len(article_urls):\n",
    "            print(f\"   {i:>3}/{len(article_urls)} pages done \"\n",
    "                  f\"({len(docs)} paragraphs)\")\n",
    "\n",
    "        time.sleep(DELAY_SEC)\n",
    "\n",
    "    # save outputs\n",
    "    OUT_JSONL.write_text(\"\\n\".join(json.dumps(d, ensure_ascii=False) for d in docs),\n",
    "                         encoding=\"utf-8\")\n",
    "    OUT_ART_INDEX.write_text(json.dumps(article_urls, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\n✅  Finished.\")\n",
    "    print(f\"   Pages crawled  : {len(article_urls)}\")\n",
    "    print(f\"   Paragraphs kept: {len(docs)}\")\n",
    "    print(f\"   JSONL saved to : {OUT_JSONL.relative_to(pathlib.Path.cwd())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37e1444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍  Reading robots.txt …\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No Sitemap lines found in robots.txt – Stripe may have changed their layout.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m, in \u001b[0;36mcrawl\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrawl\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     sitemap_urls \u001b[38;5;241m=\u001b[39m \u001b[43mget_sitemap_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     article_urls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(iter_article_urls(sitemap_urls)))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Discovered \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(article_urls)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m article URLs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m, in \u001b[0;36mget_sitemap_urls\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         sm_urls\u001b[38;5;241m.\u001b[39mappend(url)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sm_urls:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Sitemap lines found in robots.txt – \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStripe may have changed their layout.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sm_urls)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sitemap file(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sm_urls\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No Sitemap lines found in robots.txt – Stripe may have changed their layout."
     ]
    }
   ],
   "source": [
    "crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37965c3f",
   "metadata": {},
   "source": [
    "FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfecdb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1662/5278596.py:26: UserWarning: Model 'nomic-ai/nomic-embed-text-v1' failed: Loading nomic-ai/nomic-embed-text-v1 requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.\n",
      "  warnings.warn(f\"Model '{name}' failed: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Loaded 'intfloat/e5-base-v2' in 21.2s\n",
      "Embedding 5,894 paragraphs …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:09<00:00,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector matrix: (5894, 768)\n",
      "🎉  FAISS index saved → artifacts/faiss.idx | meta → artifacts/meta.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, numpy as np, faiss, warnings, time\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# -------- paths -----------------------------------------------------------\n",
    "DATA_PATH = pathlib.Path(\"data/raw/stripe_faqs_full.jsonl\")\n",
    "ART_DIR   = pathlib.Path(\"artifacts\"); ART_DIR.mkdir(exist_ok=True)\n",
    "IDX_FILE  = ART_DIR / \"faiss.idx\"\n",
    "META_FILE = ART_DIR / \"meta.npy\"\n",
    "\n",
    "# -------- choose an available embedding model ----------------------------\n",
    "CANDIDATES = [\n",
    "    \"nomic-ai/nomic-embed-text-v1\",\n",
    "    \"intfloat/e5-base-v2\",\n",
    "    \"thenlper/gte-base\",\n",
    "]\n",
    "\n",
    "model = None\n",
    "for name in CANDIDATES:\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        model = SentenceTransformer(name)\n",
    "        print(f\"✅  Loaded '{name}' in {time.time()-t0:.1f}s\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Model '{name}' failed: {e}\")\n",
    "\n",
    "if model is None:\n",
    "    raise RuntimeError(\"No embedding model could be loaded. Check internet / HF token.\")\n",
    "\n",
    "# -------- load texts ------------------------------------------------------\n",
    "texts = []\n",
    "with DATA_PATH.open() as f:\n",
    "    for line in f:\n",
    "        texts.append(json.loads(line)[\"text\"])\n",
    "\n",
    "print(f\"Embedding {len(texts):,} paragraphs …\")\n",
    "\n",
    "# -------- embed in batches -----------------------------------------------\n",
    "vecs = []\n",
    "for start in tqdm(range(0, len(texts), 256)):\n",
    "    batch = texts[start:start+256]\n",
    "    vec   = model.encode(batch,\n",
    "                         batch_size=64,\n",
    "                         normalize_embeddings=True,\n",
    "                         show_progress_bar=False)\n",
    "    vecs.append(vec.astype(\"float32\"))\n",
    "\n",
    "vecs = np.vstack(vecs)\n",
    "print(\"Vector matrix:\", vecs.shape)\n",
    "\n",
    "# -------- build & save FAISS ---------------------------------------------\n",
    "index = faiss.IndexFlatIP(vecs.shape[1])\n",
    "index.add(vecs)\n",
    "faiss.write_index(index, str(IDX_FILE))\n",
    "np.save(META_FILE, np.array(texts, dtype=object))\n",
    "\n",
    "print(\"🎉  FAISS index saved →\", IDX_FILE, \"| meta →\", META_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb68e9e9",
   "metadata": {},
   "source": [
    "Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb32223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, k: int = 4):\n",
    "    q_vec = model.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "    scores, idxs = index.search(q_vec, k)\n",
    "    return [(texts[i], scores[0][j], urls[i]) for j, i in enumerate(idxs[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6afad394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(user_q: str, ctx_chunks):\n",
    "    bullet_list = \"\\n\".join(f\"- {c[0]}\" for c in ctx_chunks)\n",
    "    return (\n",
    "        f\"Answer the user question using only the context snippets. \"\n",
    "        f\"Respond in markdown.\\n\\n\"\n",
    "        f\"### Context:\\n{bullet_list}\\n\\n\"\n",
    "        f\"### Question: {user_q}\\n\\n### Answer:\"\n",
    "    )\n",
    "\n",
    "def ollama(prompt: str):\n",
    "    payload = {\"model\": LLM_MODEL, \"prompt\": prompt, \"stream\": False}\n",
    "    r = requests.post(f\"{OLLAMA_URL}/api/generate\", json=payload, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"response\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d12bebab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Answer:\n",
      " **7-14 days after you successfully receive your first payment**, depending on your industry risk level and country of operation. If this is your first payout, it might take longer due to the initial delay to mitigate risk. \n",
      "\n",
      "## Sources:\n",
      "• If this is your first payout, Stripe typically schedules your initial payout for 7-14 days after you successfully [...] → https://support.stripe.com/questions/where-is-my-payout-faq-for-late-and-missing-payouts\n",
      "• If this is your first payout, Stripe typically schedules your initial payout for 7-14 days after you successfully [...] → https://support.stripe.com/questions/where-is-my-payout-faq-for-missing-and-late-payouts\n",
      "• The first payout for every new Stripe account is made seven days after the first successful payment is received. [...] → https://support.stripe.com/questions/getting-started-with-stripe-through-a-third-party-platform\n",
      "• There is a seven day waiting period for the first payout. Several factors can extend the waiting period, including [...] → https://support.stripe.com/questions/payout-schedule-for-stripe-accounts-in-india\n"
     ]
    }
   ],
   "source": [
    "q = \"When will I receive my first payout on Stripe?\"\n",
    "ctx = search(q, k=4)\n",
    "prompt = build_prompt(q, ctx)\n",
    "answer = ollama(prompt)\n",
    "\n",
    "print(\"## Answer:\\n\", answer, \"\\n\")\n",
    "print(\"## Sources:\")\n",
    "for txt, score, url in ctx:\n",
    "    print(\"•\", textwrap.shorten(txt, 120), \"→\", url)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
